{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handwritten Equation solver using CNN\n",
    "### **Nitish M. Satheesh**\n",
    "\n",
    "`Python version 3.9.0`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports & stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.applications.mobilenet import MobileNet, preprocess_input # type: ignore\n",
    "from tensorflow.keras.models import Model # type: ignore\n",
    "from tensorflow.keras.layers import Dense, Input # type: ignore\n",
    "from tensorflow.keras.optimizers import Adam # type: ignore\n",
    "from tensorflow.keras.utils import to_categorical # type: ignore\n",
    "from vit_keras import vit, utils\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to dataset\n",
    "BASE_PATH = './aida'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "**Load and Parse JSON Annotations to extract the annotations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_annotations(batch_num):\n",
    "    json_path = os.path.join(BASE_PATH, f'batch_{batch_num}', 'JSON', f'kaggle_data_{batch_num}.json')\n",
    "    with open(json_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    print(f\" [*] Loaded annotations 'JSON\\\\kaggle_data_{batch_num}.json'\")\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Images**\n",
    "\n",
    "Next, load the images from the `background_images` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(batch_num):\n",
    "    image_folder = os.path.join(BASE_PATH, f'batch_{batch_num}', 'background_images')\n",
    "    images = []\n",
    "    image_paths = []\n",
    "    print(f\"\\n [---] Loading images from 'batch_{batch_num}\\\\background_images'\", end=\" \")\n",
    "    for filename in tqdm(os.listdir(image_folder)):\n",
    "        img_path = os.path.join(image_folder, filename)\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "            image_paths.append(img_path)\n",
    "    print(f\" [*] Loaded {len(image_paths)} images.\")\n",
    "    return images, image_paths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocess Images**\n",
    "\n",
    "Resize and normalize the images to fit the input requirements of MobileNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images(images, target_size=(224, 224)):\n",
    "    processed_images = []\n",
    "    print(\"\\n[---] Preprocessing images\", end=\" \")\n",
    "    for img in tqdm(images):\n",
    "        img_resized = cv2.resize(img, target_size)\n",
    "        img_preprocessed = utils.preprocess_inputs(img_resized) \n",
    "        processed_images.append(img_preprocessed)\n",
    "    print(\"[*] Preprocessing images done.\")\n",
    "    return np.array(processed_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_data(batch_nums):\n",
    "#     all_images = []\n",
    "#     all_labels = []\n",
    "#     all_image_paths = []\n",
    "\n",
    "#     for batch_num in batch_nums:\n",
    "#         annotations = load_annotations(batch_num)\n",
    "#         images, image_paths = load_images(batch_num)\n",
    "        \n",
    "#         for img, img_path in zip(images, image_paths):\n",
    "#             img_filename = os.path.basename(img_path)\n",
    "#             if img_filename in annotations:\n",
    "#                 all_images.append(img)\n",
    "#                 all_labels.append(annotations[img_filename]['label'])\n",
    "#                 all_image_paths.append(img_path)\n",
    "\n",
    "#     processed_images = preprocess_images(all_images)\n",
    "#     return processed_images, all_labels, all_image_paths\n",
    "\n",
    "def prepare_data(batch_nums):\n",
    "    all_images = []\n",
    "    all_labels = []\n",
    "    all_image_paths = []\n",
    "\n",
    "    for batch_num in batch_nums:\n",
    "        print(f\"\\n[---] Processing batch {batch_num}\")\n",
    "        annotations = load_annotations(batch_num)\n",
    "        images, image_paths = load_images(batch_num)\n",
    "        \n",
    "        print(f\" [*] Found {len(images)} images and {len(annotations)} annotations\")\n",
    "        \n",
    "        # Create a dictionary mapping filenames to annotations for faster lookup\n",
    "        annotation_dict = {item['filename']: item for item in annotations}\n",
    "        \n",
    "        # Print first 5 image filenames and first 3 annotation filenames\n",
    "        print(f\" [*] Sample image filenames: {[os.path.basename(path) for path in image_paths[:3]]}\")\n",
    "        print(f\" [*] Sample annotation filenames: {[item['filename'] for item in annotations[:3]]}\")\n",
    "        \n",
    "        matched_count = 0\n",
    "        unmatched_count = 0\n",
    "        for img, img_path in tqdm(zip(images, image_paths)):\n",
    "            img_filename = os.path.basename(img_path)\n",
    "            if img_filename in annotation_dict:\n",
    "                latex = annotation_dict[img_filename].get('latex')\n",
    "                if latex:  # Check if latex is not None or empty string\n",
    "                    all_images.append(img)\n",
    "                    all_labels.append(latex)\n",
    "                    all_image_paths.append(img_path)\n",
    "                    matched_count += 1\n",
    "                else:\n",
    "                    unmatched_count += 1\n",
    "                    print(f\" [!] Image {img_filename} found in annotations but has no latex\")\n",
    "            else:\n",
    "                unmatched_count += 1\n",
    "                if unmatched_count <= 5:  # Print only first 5 unmatched files to avoid cluttering the output\n",
    "                    print(f\" [!] Image {img_filename} not found in annotations\")\n",
    "        \n",
    "        print(f\"  [>] Matched {matched_count} images with annotations\")\n",
    "        print(f\"  [>] Unmatched {unmatched_count} images\")\n",
    "\n",
    "    print(f\"[*] Total processed images: {len(all_images)}\")\n",
    "    print(f\"[*] Total labels: {len(all_labels)}\")\n",
    "    print(f\"[*] Sample labels: {all_labels[:5]}\")  # Print first 5 labels for inspection\n",
    "\n",
    "    if not all_labels:\n",
    "        raise ValueError(\"[!] No valid labels found in the dataset\")\n",
    "\n",
    "    processed_images = preprocess_images(all_images)\n",
    "    return processed_images, all_labels, all_image_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME: Change range to (1, 11). Using only batch 1 for now\n",
    "BATCH_NUMS = range(1, 2)\n",
    "\n",
    "# Prepare data\n",
    "X, y, image_paths = prepare_data(BATCH_NUMS)\n",
    "\n",
    "print(f\"[INFO] Shape of X: {X.shape}\")\n",
    "print(f\"[INFO] Length of y: {len(y)}\")\n",
    "\n",
    "# Get unique labels and create a label-to-index mapping\n",
    "unique_labels = list(set(y))\n",
    "print(f\"[INFO] Number of unique labels: {len(unique_labels)}\")\n",
    "print(f\"[INFO] Sample unique labels: {unique_labels[:5]}\")  # Print first 5 unique labels\n",
    "\n",
    "label_to_index = {label: index for index, label in enumerate(unique_labels)}\n",
    "\n",
    "# Convert string labels to indices\n",
    "y_indices = [label_to_index[label] for label in y]\n",
    "\n",
    "print(f\"[INFO] Sample y_indices: {y_indices[:5]}\")  # Print first 5 indices\n",
    "\n",
    "# Convert to one-hot encoding\n",
    "num_classes = len(unique_labels)\n",
    "y_one_hot = to_categorical(y_indices, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training MobileNet model\n",
    "\n",
    "\n",
    "**Methods for defining, compiling and training the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_classes):\n",
    "    vit_model = vit.vit_b16(\n",
    "        image_size=224,\n",
    "        activation='softmax',\n",
    "        pretrained=True,\n",
    "        include_top=False,\n",
    "        pretrained_top=False,\n",
    "        classes=num_classes\n",
    "    )\n",
    "    \n",
    "    inputs = Input(shape=(224, 224, 3))\n",
    "    x = vit_model(inputs)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_model(model, X_train, y_train, epochs=10, batch_size=32):\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.0001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the model\n",
    "model = create_model(num_classes)\n",
    "train_model(model, X, y_one_hot)\n",
    "\n",
    "# Save the model\n",
    "model.save('models/aida_vit_model.keras')\n",
    "\n",
    "# Save the label mapping\n",
    "with open('label_mapping.json', 'w') as f:\n",
    "    json.dump(label_to_index, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compile the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.optimizers import Adam\n",
    "# model.compile(optimizer=Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "\n",
    "**Prepare Data for Training**\n",
    "\n",
    "Convert annotations to a format suitable for training (e.g., one-hot encoding for classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# # Assuming annotations contain class labels\n",
    "# labels = [annotation['label'] for annotation in annotations]\n",
    "# labels_one_hot = to_categorical(labels, num_classes=num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "# model.fit(processed_images, labels_one_hot, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "# model.evaluate(validation_images, validation_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
